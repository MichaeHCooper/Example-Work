{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Three \n",
    "\n",
    "The primary description of this coursework is available on the CM20252 Moodle page. This is the Jupyter notebook you must complete and submit to receive marks. This notebook adds additional detail to the coursework specification but does not repeat the information that has already been provided there. \n",
    "\n",
    "You must follow all instructions given in this notebook precisely.\n",
    "\n",
    "Restart the kernel and run all cells before submitting the notebook. This will guarantee that we will be able to run your code for testing. Remember to save your work regularly.\n",
    "\n",
    "__You will develop players for Connect-Three on a grid that is 5 columns wide and 3 rows high. An example is shown below showing a win for Player Red.__\n",
    "\n",
    "<img src=\"images/connect3.png\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "For your reference, below is a visual depiction of the agent-environment interface in reinforcement learning. The interaction of the agent with its environments starts at decision stage $t=0$ with the observation of the current state $s_0$. (Notice that there is no reward at this initial stage.) The agent then chooses an action to execute at decision stage $t=1$. The environment responds by changing its state to $s_1$ and returning the numerical reward signal $r_1$. \n",
    "\n",
    "<img src=\"images/agent-environment.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "Below, we provide some code that will be useful for implementing parts of this interface. You are not obligated to use this code; please feel free to develop your own code from scratch. \n",
    "\n",
    "### Code details\n",
    "\n",
    "We provide a `Connect` class that you can use to simulate Connect-Three games. The following cells in this section will walk you through the basic usage of this class by playing a couple of games.\n",
    "\n",
    "We import the `connect` module and create a Connect-Three environment called `env`. The constructor method has one argument called `verbose`. If `verbose=True`, the `Connect` object will regularly print the progress of the game. This is useful for getting to know the provided code, debugging your code, or if you just want to play around. You will want to set `verbose=False` when you run hundreds of episodes to complete the marked exercises.\n",
    "\n",
    "This `Connect` environment uses the strings `'o'` and `'x'` instead of different disk colors in order to distinguish between the two players. We can specify who should start the game using the `starting_player` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game has been reset.\n",
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "import connect\n",
    "env = connect.Connect(starting_player='x', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can interact with the environment using the `act()` method. This method takes an `action` (an integer) as input and computes the response of the environment. An action is defined as the column index that a disk is dropped into. The `act()` method returns the `reward` for player `'o'` and a boolean, indicating whether the game is over (`True`) or not (`False`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']]\n",
      "reward = 0\n",
      "game_over = False\n"
     ]
    }
   ],
   "source": [
    "reward, game_over = env.act(action=2)\n",
    "print(\"reward =\", reward)\n",
    "print(\"game_over =\", game_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we set `verbose=True` when we created our environment, the grid is printed each time we call the `act()` method. You probably might want to set `verbose=False` when you run Q-learning for thousands of episodes. \n",
    "\n",
    "As expected, the `reward` is 0 and no one has won the game yet (`game_over` is `False`). Let us drop another disk into the same column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "reward, game_over = env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the `Connect` environment automatically switches the\n",
    "\n",
    "The `grid` is stored as a two-dimensional `numpy` array in the `Connect` class and you can easily access it by calling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "current_grid = env.grid\n",
    "print(current_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the grid now appears to be \"upside down\" because `numpy` arrays are printed from \"top to bottom\".\n",
    "We can also print it the way it is printed by the Connect class by calling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "print(current_grid[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make another move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "reward, game_over = env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to put another disk in the same column with `act(action=2)`. The environment will throw an error because that column is already filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-08cd2c81a094>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# This cell should throw an IndexError!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mH:\\ReinforcementLearning\\AICoursework2\\connect.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \"\"\"\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lowest_free_row_per_column\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactive_player\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lowest_free_row_per_column\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m  \u001b[1;31m# You can ignore this; internal use only.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lowest_free_row_per_column\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_rows\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "# This cell should throw an IndexError!\n",
    "env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `.available_actions` of the `Connect` class contains a `numpy` array of all not yet filled columns. This variable should help you to avoid errors like the one we have just encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(env.available_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that column index '2' is missing because this column is already filled.\n",
    "\n",
    "Let's keep on playing until some player wins..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' 'o' ' ']]\n",
      "reward = 0 game_over = False\n",
      "[[' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']\n",
      " [' ' 'x' 'x' 'o' ' ']]\n",
      "reward = 0 game_over = False\n",
      "[[' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' 'o' 'o' ' ']\n",
      " [' ' 'x' 'x' 'o' ' ']]\n",
      "reward = 0 game_over = False\n",
      "[[' ' ' ' 'x' ' ' ' ']\n",
      " [' ' 'x' 'o' 'o' ' ']\n",
      " [' ' 'x' 'x' 'o' ' ']]\n",
      "reward = 0 game_over = False\n",
      "[[' ' ' ' 'x' 'o' ' ']\n",
      " [' ' 'x' 'o' 'o' ' ']\n",
      " [' ' 'x' 'x' 'o' ' ']]\n",
      "Player ' o ' has won the game!\n",
      "reward = 1 game_over = True\n"
     ]
    }
   ],
   "source": [
    "reward, game_over = env.act(action=3)\n",
    "print(\"reward =\", reward, \"game_over =\", game_over) \n",
    "reward, game_over = env.act(action=1)\n",
    "print(\"reward =\", reward, \"game_over =\", game_over)\n",
    "reward, game_over = env.act(action=3)\n",
    "print(\"reward =\", reward, \"game_over =\", game_over)\n",
    "reward, game_over = env.act(action=1)\n",
    "print(\"reward =\", reward, \"game_over =\", game_over)\n",
    "reward, game_over = env.act(action=3)\n",
    "print(\"reward =\", reward, \"game_over =\", game_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the `reward` returned by the `act()` method is the reward for player `'o'`.\n",
    "\n",
    "You can reset the game using the `reset()` method. This method cleans the grid and makes sure that the it is the `starting_player`'s turn as defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game has been reset.\n",
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']]\n",
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' 'x' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "reward, game_over = env.act(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to modify existing or add new methods to the `Connect` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "**Your opponent is always the first player. Your agent is always the second player.**\n",
    "\n",
    "For your reference, the pseudo-code for Q-learning is reproduced below from the textbook (Reinforcement Learning, Sutton & Barto, 1998, Section 6.5).\n",
    "<img src=\"images/q_learning.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "Prepare a **learning curve** following the directions below. We refer to this as Plot 1.\n",
    "\n",
    "After $n$ steps of interaction with the environment, play $m$ games with the current policy of the agent (without modifying the policy). Think of this as interrupting the agent for a period of time to test how well it has learned so far. Your plot should show the total score obtained in these $m$ games as a function of $n, 2n, 3n, … kn$. The choices of $n$ and $k$ are up to you. They should be reasonable values that demonstrate the efficiency of the learning and how well the agent learns to play the game eventually. Use $m=10$. \n",
    "\n",
    "This plot should show the mean performance of `a` agents, not the performance of a single agent. Because of the stochasticity in the environment, you will obtain two different learning curves from two different agents even though they are using exactly the same algorithm. We suggest setting `a` to 20 or higher.\n",
    "\n",
    "Present a single mean learning curve with your choice of parameters $\\epsilon$ and $\\alpha$. The plot should also show (as a baseline) the mean performance of a random agent that does not learn but chooses actions uniformly randomly from among the legal actions. Label this line “Random Agent”. \n",
    "\n",
    "Please include this plot as a static figure in the appropriate cell below. That is, compute the learning curve in the lab or at home (this may take a couple of minutes depending on your implementation) and save the figure in the same directory as your notebook. Import this figure in the appropriate answer cell under (A). You can look at the source code of this markdown cell (double click on it!) to find out how to embed figures using html. Do **not** use drag & drop to include figures; we would not be able to see them! Make sure to include the locally stored images in your submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1431aa87b9e9019a4dbe6e696e0a9082",
     "grade": true,
     "grade_id": "cell-3ac2114f764e8410",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Although python doesn't really have abstract classes, creating a class to inherit from is useful as it means that I have a\n",
    "    reference point of what functions must be included.  This class represents a general agent with a move function which takes\n",
    "    the current availible actions and decides what move to take next.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Class constructor.\n",
    "        \"\"\"\n",
    "\n",
    "    def move(self, envActions):\n",
    "        \"\"\"\n",
    "        Method to determine the next move the agent will play given the availible actions object.\n",
    "        \"\"\"\n",
    "        return index # index is the position in the grid to play at.\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    \"\"\"\n",
    "    This implements the parent class, Agent, to create a random agent which randomly selects a move to take from the avalible\n",
    "    moves.  As specified a random agent if playing first will\n",
    "    \"\"\"\n",
    "    def move(self, NC):\n",
    "        \"\"\"\n",
    "        Returns a random but valid move.\n",
    "        \"\"\"\n",
    "        index = np.random.randint(0,np.shape(NC.emptyCells)[0])\n",
    "        # Decides on an index to choose from in the empty cells over the length of the empty cells.\n",
    "        moveIndex = NC.emptyCells[index]\n",
    "        # Decides on a move by outputting the index of the position in the board at which a marke should be put.\n",
    "        return moveIndex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "980d73fb62fae59d610abc96121f71bc",
     "grade": true,
     "grade_id": "cell-ce1405b859519f91",
     "locked": false,
     "points": 60,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "(A) [continued} Insert your static learning curve here (Plot 1).\n",
    "\n",
    "YOUR ANSWER HERE \n",
    "\n",
    "(B) In 3 sentences or less, explain your conclusions from the plot above. How close does your (average) agent get to the best possible level of performance? How efficiently does your (average) agent learn? \n",
    "\n",
    "YOUR ANSWER HERE \n",
    "\n",
    "(C) In five sentences or less, explain the key aspects of your implementation. How many state-action pairs do you represent in your Q-table? Describe and justify your settings of $\\alpha$ and $\\epsilon$. Are there any things you tried out that are not in your final implementation?\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "(D) In the cell below, make it possible for us to produce from scratch a learning curve similar to Plot 1 but for a single agent, for a $k$ value of your own choosing. You do not need to include the baseline for random play.  This code should run in less than 30 seconds (ours runs in 2 seconds). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e65915a61d304027e4fbd2e714c4beba",
     "grade": true,
     "grade_id": "cell-e0e01e05236aee45",
     "locked": false,
     "points": 40,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### This cell should produce from scratch a plot showing a learning curve for a single agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT: How to submit.\n",
    "\n",
    "If any of the following instructions is not clear, please ask your tutors well ahead of the submission deadline.\n",
    "\n",
    "### Before you submit\n",
    "- We will not be able to mark your coursework if it takes more than 1 minutes to execute your entire notebook. That is, comment out (but do not delete) the code that you used to produce Plot 1 (i.e., learning curve averaged across many agents). Do **not** comment out the code that you use to produce a learning curve for a single agent (Exercise D).\n",
    "- Restart the kernel (_Kernel $\\rightarrow$ Restart & Run All_) and make sure that you can run all cells from top to bottom without any errors.\n",
    "- Make sure that your code is written in Python 3 (and not in Python 2!). You can check the Python version of the current session in the top-right corner below the Python logo.\n",
    "\n",
    "### Submission file\n",
    "- Please upload to Moodle a .zip file (**not** `.rar`, `.7z`, or any other archive format) that contains the completed Jupyter notebook (`ai4_connect_three.ipynb`) as well as the pre-computed figure(s). \n",
    "- **If** you change the `connect.py` file or write your own version of the environment, include the corresponding file in your submission, but give it any other name than `connect.py`. If you do not change its name, it will be overwritten  and we won't be able to execute your code! Make sure that you import the correct module when you rename your file, for example, use `import myConnect` if your file is called `myConnect.py`.\n",
    "- Do not include any identifying information. Not in the code cells, not in the file names, nowhere! Marking is anonymous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
