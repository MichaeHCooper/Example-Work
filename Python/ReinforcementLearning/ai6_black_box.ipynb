{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Zero or One? (100 marks)\n",
    "\n",
    "All you will be given about this problem is a training data set. Your objective is to develop a classifier that will have the highest accuracy in unseen examples.\n",
    "\n",
    "The following cell loads the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data set: (5000, 39)\n",
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [1. 0. 1. ... 0. 1. 0.]\n",
      " [1. 1. 1. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 1. 0.]\n",
      " [1. 0. 1. ... 0. 1. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_data = np.loadtxt(open(\"data/training_data.csv\"), delimiter=\",\")\n",
    "print(\"Shape of the training data set:\", training_data.shape)\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is again the response variable. The remaining 38 columns are binary features. You have multiple tasks:\n",
    "\n",
    "(1) Your first task is to write a function called `train()` that takes `training_data` as input and returns all the fitted parameters of your model. Note that the fitted parameters of your model depend on the model you choose. For example, if you use a na√Øve Bayes classifier, you could return a list of class priors and conditional likelihoods. (This function will allow us to compute your model on the fly. We should be able to execute it in less than 10 minutes.) \n",
    "\n",
    "(2) Your second task is to provide a variable called `fitted_model` which stores the model parameters you found by executing your train() function on the training_data. If your train function takes more than 20 seconds to run, this variable should load precomputed parameter values (possibly from a file) rather than execute the train() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "############################################################# Setup #############################################################\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LeakyReLU\n",
    "from keras.losses import mean_squared_error\n",
    "\n",
    "######################################################## Neural Network #########################################################\n",
    "\n",
    "class NNclassifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Class Constructor, this initiates the model.\n",
    "        \"\"\"\n",
    "        self.model = self.buildModel()\n",
    "        # Keras network model.\n",
    "\n",
    "    def buildModel(self):\n",
    "        \"\"\"\n",
    "        Builds the model for the neural network.  The current network is the best architecture I've so far determined. I use\n",
    "        succesively smaller dense layers then add a healthy amount of dropout to prevent overfitting.  Leaky ReLU is used to\n",
    "        smooth training by preventing zero convergence.\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        # Initialises the model.\n",
    "        model.add(Dense(50, activation = 'relu', input_shape=(38,)))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(Dropout(0.4))\n",
    "        # Input Layer.\n",
    "        model.add(Dense(200, activation = 'relu'))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(100, activation = 'relu'))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(50, activation = 'relu'))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(25, activation = 'relu'))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(Dropout(0.1))\n",
    "        # Hidden Layers\n",
    "        model.add(Dense(1, activation = 'relu'))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        # Output Layer.\n",
    "        model.compile(loss=mean_squared_error, optimizer=keras.optimizers.Adam())\n",
    "        # Compiling model.\n",
    "        return model\n",
    "\n",
    "    def predict(self, testDataX, threshold = 0.5):\n",
    "        \"\"\"\n",
    "        This makes a prediction from a set of data.  Takes in a batch of dimensions (batchSize, 38) returns an array of\n",
    "        dimensions (batchSize)\n",
    "        \"\"\"\n",
    "        testDataX = np.array(testDataX, dtype = float)\n",
    "        # Ensuring the incoming data is infact a float.\n",
    "        predDataY = self.model.predict(testDataX)\n",
    "        # Making the predictions.\n",
    "        thresholdArray = np.zeros(np.shape(predDataY))+threshold\n",
    "        predDataY = np.greater(predDataY, thresholdArray)#, dtype = int)\n",
    "        # Deciding if predicted data is above or below threshold.\n",
    "        return predDataY[:,0]\n",
    "\n",
    "    def accuracy(self, predDataY, testDataY):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy of the prediction.  Takes two arrays both of dimension (batchSize) returns a float.\n",
    "        \"\"\"\n",
    "        yPred = np.array(predDataY, dtype = int)\n",
    "        yTrue = np.array(testDataY, dtype = int)\n",
    "        # Ensuring the data is correctly cast.\n",
    "        rows = np.shape(yPred)\n",
    "        # Calculates the number of messages in data.\n",
    "        equal = np.equal(yPred, yTrue, dtype=int)\n",
    "        # Works out element wise if the two arrays are equal.\n",
    "        numEqual = np.sum(equal)\n",
    "        # Calculates the number that are equal.\n",
    "        accuracy = numEqual/rows\n",
    "        # Divides the number equal by the number of rows to get the proportion.\n",
    "        return accuracy[0]\n",
    "\n",
    "    def train(self, trainData, testData, epochs, batchSize, verbose = False):\n",
    "        \"\"\"\n",
    "        Trains the network.  Takes two arrays, the trainingData and the testingData both of dimensions (batchsize, 39).\n",
    "        \"\"\"\n",
    "        trainDataX = np.array(trainData[:,1:], dtype = float)\n",
    "        trainDataY = np.array(trainData[:,0] , dtype = int)\n",
    "        # Training Data.\n",
    "        testDataX  = np.array(testData[:,1:], dtype = float)\n",
    "        testDataY  = np.array(testData[:,0] , dtype = int  )\n",
    "        # Validation Data.\n",
    "        #learningCurve = []\n",
    "        # Learning curve.\n",
    "        for i in range(epochs):\n",
    "            # Iterating through training epochs.\n",
    "            self.model.fit(x=trainDataX, y=trainDataY, batch_size=batchSize, epochs=1, verbose = 0)\n",
    "            # Fits the model.\n",
    "            #predDataY = self.predict(testDataX)\n",
    "            #loss = self.accuracy(predDataY, testDataY)\n",
    "            #learningCurve.append(loss)\n",
    "            # Creates the learning curve.\n",
    "            #if verbose and (i%50==0):\n",
    "            #    print(\"Episode \"+str(i)+\" : Accuracy \"+str(loss))\n",
    "        #return learningCurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "98502b22b7569c3888bbea2dae89e8a6",
     "grade": false,
     "grade_id": "cell-9284ab8e9721ffc2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train(training_data):\n",
    "    \"\"\"\n",
    "    Train a model on the training_data\n",
    "\n",
    "    :param training_data: a two-dimensional numpy-array with shape = [5000, 39] \n",
    "    \n",
    "    :return fitted_model: any data structure that captures your model\n",
    "    \"\"\"\n",
    "    agent = NNclassifier()\n",
    "    agent.train(training_data, training_data, 300, 1000, verbose = True)\n",
    "    return agent\n",
    "\n",
    "## Uncomment one of the following two lines depending on whether you want us to compute your model on the \n",
    "## fly or load a supplementary file.\n",
    "\n",
    "fitted_model = train(training_data)\n",
    "\n",
    "# fitted_model = load(local_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Your third task is to provide a function called `test()` that uses your `fitted_model` to classify the observations of `testing_data`. The `testing_data` is hidden and may contain any number of observations (rows). It contains 38 columns that have the same structure as the features of `training_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8547b4673b0548ddd786722f367268a4",
     "grade": false,
     "grade_id": "cell-b62974b058e95e23",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def test(testing_data, fitted_model):\n",
    "    \"\"\"\n",
    "    Classify the rows of testing_data using a fitted_model. \n",
    "\n",
    "    :param testing_data: a two-dimensional numpy-array with shape = [n_test_samples, 38]\n",
    "    :param fitted_model: the output of your train function.\n",
    "\n",
    "    :return class_predictions: a numpy array containing the class predictions for each row\n",
    "        of testing_data.\n",
    "    \"\"\"\n",
    "    agent = fitted_model\n",
    "    predictions = agent.predict(testing_data)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8e632be3b415f26b688aaf8da8354865",
     "grade": true,
     "grade_id": "cell-d6503e588628ad10",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell. Do not delete or change. \n",
    "# You can use this cell to check whether the returned objects of your function are of the right data type.\n",
    "\n",
    "# Test data types if input are the first 20 rows of the training_data.\n",
    "class_predictions = test(training_data[:20, 1:], fitted_model)\n",
    "\n",
    "# Check data type(s)\n",
    "assert(isinstance(class_predictions, np.ndarray))\n",
    "\n",
    "# Check shape of numpy array\n",
    "assert(class_predictions.shape == (20,))\n",
    "\n",
    "# Check data type of array elements\n",
    "assert(np.all(np.logical_or(class_predictions == 0, class_predictions == 1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7dcae32826a9c82b4897eec6dc9470b6",
     "grade": true,
     "grade_id": "cell-1d336f7ffecd8f71",
     "locked": false,
     "points": 100,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Describe in less than 10 sentences: Explain your classifier. Comment on its performance. What other alternative classifiers did you consider or experiment with? How does the performance of your classifier change as the size of the training set increases? You may want to include figures. \n",
    "\n",
    "I have used a basic neural network using dense layers, typically called a multilayer perceptron.  I used progressively smaler hidden layers from the first, this typically creates a stable network, furthermore I included significant dropout especially in the earlier layers to prevent overfitting.  I used Leaky ReLU activations, ReLU activations are computationally effecient and stable, tending to learn well and not overfit; by using leaky ReLU activations the network doesn't get caught converging weights entirely to zero.  Finally I use the ADAM optimiser, this is effecient and automatically calculates hyper-parameters.\n",
    "\n",
    "The network performs well and averages at maximum 0.967 accuracy given a 3:1 training to testing split of the data.  Altough I attempted many different varients of network architectures, batch sizes and activations this network achieved the highest accuracy.  Interestingly using a mean squared error loss proved more accurate than categorical cross entropy, an unusaual result but likely due to the higher stability of MSE.\n",
    "\n",
    "Determing the results of training on more data was difficult; by increasing the size of the training-testing split the network learnt better but was much more suceptible to outliers in the ever smaller testing set leading to noisier learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
